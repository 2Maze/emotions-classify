{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a8ccd12-05a9-4067-84ba-2dd3d9436725",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.data import EmotionDataset\n",
    "from torchaudio.transforms import MelSpectrogram"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cca5636-56e5-47bd-b095-e34e62ee0c56",
   "metadata": {},
   "source": [
    "transforms = nn.Sequential(MelSpectrogram(n_mels=80))\n",
    "\n",
    "dataset = EmotionDataset('data/dataset/annotations.json', padding_sec=30, transforms=transforms)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3234d057-75f0-42df-b441-0753dd706901",
   "metadata": {},
   "source": [
    "def pad_or_trim(array, length: int = 3000, *, axis: int = -1):\n",
    "    \"\"\"\n",
    "    Pad or trim the audio array to N_SAMPLES, as expected by the encoder.\n",
    "    \"\"\"\n",
    "    if torch.is_tensor(array):\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.index_select(\n",
    "                dim=axis, index=torch.arange(length, device=array.device)\n",
    "            )\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = F.pad(array, [pad for sizes in pad_widths[::-1] for pad in sizes])\n",
    "    else:\n",
    "        if array.shape[axis] > length:\n",
    "            array = array.take(indices=range(length), axis=axis)\n",
    "\n",
    "        if array.shape[axis] < length:\n",
    "            pad_widths = [(0, 0)] * array.ndim\n",
    "            pad_widths[axis] = (0, length - array.shape[axis])\n",
    "            array = np.pad(array, pad_widths)\n",
    "\n",
    "    return array"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "077dc5f1-277d-4f6a-a19c-f6716895025e",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "pad_or_trim(dataset[0]['array'][0]).shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f19a86a3-8d06-4ec0-b4b8-e7233dc051b8",
   "metadata": {},
   "source": [
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "class Conv1d(nn.Conv1d):\n",
    "    def _conv_forward(\n",
    "        self, x: Tensor, weight: Tensor, bias: Optional[Tensor]\n",
    "    ) -> Tensor:\n",
    "        return super()._conv_forward(\n",
    "            x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype)\n",
    "        )\n",
    "\n",
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.query = Linear(n_state, n_state)\n",
    "        self.key = Linear(n_state, n_state, bias=False)\n",
    "        self.value = Linear(n_state, n_state)\n",
    "        self.out = Linear(n_state, n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ):\n",
    "        q = self.query(x)\n",
    "\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n",
    "            # otherwise, perform key/value projections for self- or cross-attention as usual.\n",
    "            k = self.key(x if xa is None else xa)\n",
    "            v = self.value(x if xa is None else xa)\n",
    "        else:\n",
    "            # for cross-attention, calculate keys and values once and reuse in subsequent calls.\n",
    "            k = kv_cache[self.key]\n",
    "            v = kv_cache[self.value]\n",
    "\n",
    "        wv, qk = self.qkv_attention(q, k, v, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(\n",
    "        self, q: Tensor, k: Tensor, v: Tensor, mask: Optional[Tensor] = None\n",
    "    ):\n",
    "        n_batch, n_ctx, n_state = q.shape\n",
    "        scale = (n_state // self.n_head) ** -0.25\n",
    "        q = q.view(*q.shape[:2], self.n_head, -1).permute(0, 2, 1, 3) * scale\n",
    "        k = k.view(*k.shape[:2], self.n_head, -1).permute(0, 2, 3, 1) * scale\n",
    "        v = v.view(*v.shape[:2], self.n_head, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "        qk = q @ k\n",
    "        if mask is not None:\n",
    "            qk = qk + mask[:n_ctx, :n_ctx]\n",
    "        qk = qk.float()\n",
    "\n",
    "        w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, n_state: int, n_head: int, cross_attention: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadAttention(n_state, n_head)\n",
    "        self.attn_ln = LayerNorm(n_state)\n",
    "\n",
    "        self.cross_attn = (\n",
    "            MultiHeadAttention(n_state, n_head) if cross_attention else None\n",
    "        )\n",
    "        self.cross_attn_ln = LayerNorm(n_state) if cross_attention else None\n",
    "\n",
    "        n_mlp = n_state * 4\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(n_state, n_mlp), nn.GELU(), Linear(n_mlp, n_state)\n",
    "        )\n",
    "        self.mlp_ln = LayerNorm(n_state)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        xa: Optional[Tensor] = None,\n",
    "        mask: Optional[Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "    ):\n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
    "        if self.cross_attn:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x\n",
    "\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_mels: int, n_ctx: int, n_state: int, n_head: int, n_layer: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv1d(n_mels, n_state, kernel_size=3, padding=1)\n",
    "        self.conv2 = Conv1d(n_state, n_state, kernel_size=3, padding=1)\n",
    "        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, n_state))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(n_state, n_head) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_post = LayerNorm(n_state)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)\n",
    "            the mel spectrogram of the audio\n",
    "        \"\"\"\n",
    "        print(x.shape)\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        print(x.shape)\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        print(x.shape)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(x.shape)\n",
    "        print(self.positional_embedding.shape)\n",
    "        assert x.shape[1:] == self.positional_embedding.shape, \"incorrect audio shape\"\n",
    "        x = (x + self.positional_embedding).to(x.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ce1ef01-54a0-4d60-b835-32901fe81db9",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from torch.nn import Linear, LayerNorm\n",
    "encoder = AudioEncoder(n_mels=80, n_ctx=3000, n_state=128, n_head=4, n_layer=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6efa793b-bc79-4336-b704-184d458893f0",
   "metadata": {},
   "source": [
    "pad_or_trim(dataset[0]['array'][0])[None, None, :].shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "876ce290-82aa-43d5-8118-c604cfe66dc6",
   "metadata": {},
   "source": [
    "encoder(pad_or_trim(dataset[0]['array'][0])[None, :]).shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b179a6-67f5-4663-8b06-8ad6a4cc2eaf",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
